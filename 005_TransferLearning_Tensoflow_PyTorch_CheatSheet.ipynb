{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Transfer Learning - Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Introduction\n",
    "\n",
    "- ***Transfer learning***:  consists of taking features learned on one problem, and leveraging them on a new, similar problem.\n",
    "\n",
    "Transfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\n",
    "There are two most common ways of doing transfer learning (the other two methods are **Joint Training** and **Learning Without Forgetting** that could be read from [Zhizhong Li's paper](https://arxiv.org/abs/1606.09282))\n",
    "\n",
    "1. ***Feature Extraction***: \n",
    "    - What is it: \n",
    "        - Take a pretrained model, remove the last fully-connected layer, replace it by a new network ***block*** at the end. \n",
    "        - Freeze ***all*** of the previous layers. \n",
    "    - Explain in more details: \n",
    "        - When we train on new data set, we only train the new last layers, we don't need to retrain the entire model. \\\n",
    "        - We freeze all of the previous layers of the model but still take the activations from our frozen layers. By this, we treat these layers as as a fixed feature extractor for the new dataset since base convolutional network already contains features that are generically useful for classifying images. \n",
    "\n",
    "2. ***Fine Tuning***: \n",
    "    - What is it: \n",
    "        - Take a pretrained model, remove the last fully-connected layer, replace it by a new network ***block*** at the end (same with Feature Extraction).\n",
    "        - Freeze ***no/some*** of the previous layers. This step means we ***fine tune all/some of the weights*** of the pretrained model. \n",
    "    - Explain in more details: \n",
    "        - The first step is the same with feature extraction. \n",
    "        - In the second step, when we fine-tune all of the weights, it is the same as training them with our new data set. But instead of training the random initialized weights, we train the weights that were pretrained with the original data set.\n",
    "    - Note: \n",
    "        - When doing fine tuning some of the layers, we should \n",
    "            - **keep some of the earlier layers fixed** (due to overfitting concerns) and, \n",
    "            - only **fine-tune some higher-level portion** of the network. \n",
    "        \n",
    "        This is motivated by the observation that the earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the ConvNet becomes progressively more specific to the details of the classes contained in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. How to decide the type of transfer learning that we need to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transfer learning with Keras Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "# Freeze the convolution base\n",
    "# # setting the entire model's trainable flag to False will freeze all of the base_models' layers\n",
    "base_model.trainable = False \n",
    "# Add a classification head\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "# Now is the new model \n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you set `layer.trainable = False`, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics.\n",
    "\n",
    "When you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing `training = False` when calling the base model. Otherwise, the updates applied to the non-trainable weights will destroy what the model has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same with Feature Extraction code except we don't use base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "# Add a classification head\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "# Now is the new model \n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transfer Learning with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Feature Extraction\n",
    "\n",
    "Keywords: ***pretrained = True, named_parameters, requires_grad, fc, classifier, in_features, out_features, not freeze BatchNorm layers***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pretrained ResNet-50 model:\n",
    "transfer_model = torchvision.models.ResNet50(pretrained=True)\n",
    "#\n",
    "# Freeze the *ALL* layers by setting require_grad= False\n",
    "# Note: you might not want to freeze the BatchNorm layers in a model\n",
    "for name, param in transfer_model.named_parameters():\n",
    "    if(\"bn\" not in name):\n",
    "        param.requires_grad = False\n",
    "#\n",
    "# Then we need to replace the final classification block with a new one that we will work with your new problem\n",
    "# In this example, we replace it with a couple of Linear layers, a ReLU, and Dropout, \n",
    "# but you could have extra CNN layers here too\n",
    "# Note: PyTorch stores the final classifier *block* as an instance variable, fc or classifier  \n",
    "# so all we need to do is replace fc or classifier with our new structure\n",
    "transfer_model.fc = nn.Sequential(\n",
    "                                nn.Linear(transfer_model.fc.in_features,500),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(), \n",
    "                                nn.Linear(500,your_new_number_of_catergories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For new pytorch document: the option pretrained = TRUE has been deprecated\n",
    "\n",
    "# Create a pretrained ResNet-50 model: \n",
    "# # Reference for other options for weights: https://pytorch.org/vision/stable/models.html\n",
    "transfer_model = torchvision.models.ResNet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: **you might not want to freeze the BatchNorm layers in a model, as they will be trained to approximate the mean and standard deviation of the dataset that the model was originally trained on, not the dataset that you want to fine-tune on**. Some of the signal from your data may end up being lost as corrects your input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pretrained ResNet-50 model:\n",
    "transfer_model = torchvision.models.ResNet50(pretrained=True)\n",
    "#\n",
    "transfer_model.fc = nn.Linear(model_ft.fc.in_features, \n",
    "                                # add more layers here if you want\n",
    "                                number_of_your_problem_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other methods to boost the performance of your transfer model\n",
    "\n",
    "- Finding good learning rate: grid search, fit_one_cycle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[CS231n CNN for Visual Recognition](https://cs231n.github.io/) \\\n",
    "[Keras Transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/#do-a-round-of-finetuning-of-the-entire-model) \\\n",
    "[Pytorch transfer learning for computer vision tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) \\\n",
    "[Tensorflow Transfer learning and fine-tuning](https://www.tensorflow.org/tutorials/images/transfer_learning) \\\n",
    "\n",
    "Others (Pytorch): \\\n",
    "[https://androidkt.com/modify-pre-train-pytorch-model-for-finetuning-and-feature-extraction/](https://androidkt.com/modify-pre-train-pytorch-model-for-finetuning-and-feature-extraction/) \\\n",
    "[https://pyimagesearch.com/2021/10/11/pytorch-transfer-learning-and-image-classification/](https://pyimagesearch.com/2021/10/11/pytorch-transfer-learning-and-image-classification/) \\\n",
    "[https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) \\\n",
    "[https://harinramesh.medium.com/transfer-learning-in-pytorch-f7736598b1ed](https://harinramesh.medium.com/transfer-learning-in-pytorch-f7736598b1ed) \\\n",
    "[https://androidkt.com/modify-pre-train-pytorch-model-for-finetuning-and-feature-extraction/](https://androidkt.com/modify-pre-train-pytorch-model-for-finetuning-and-feature-extraction/) \\\n",
    "[https://www.pluralsight.com/guides/expediting-deep-learning-with-transfer-learning:-pytorch-playbook](https://www.pluralsight.com/guides/expediting-deep-learning-with-transfer-learning:-pytorch-playbook) \\\n",
    "[http://seba1511.net/tutorials/beginner/transfer_learning_tutorial.html](http://seba1511.net/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "# Need to read\n",
    "\n",
    "[https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/](https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a31d49ef82b65dcda19c5ad9b4a3838eae29e34a7ddc1c731139114b7ab7ba9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
